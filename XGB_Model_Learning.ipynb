{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RhMoxgD72ov"
   },
   "source": [
    "# Cuts Optimization using Extra Gradient Boosting\n",
    "\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXUjK-nyW4xt"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/shahidzk1/CBM_ML_Lambda_Library.git\n",
    "%cd CBM_ML_Lambda_Library\n",
    "!git pull origin main\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXNBDcG472oy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "#from hipe4ml.model_handler import ModelHandler\n",
    "#from hipe4ml.tree_handler import TreeHandler\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "#from hipe4ml import plot_utils\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from numpy import sqrt, log, argmax\n",
    "\n",
    "from google.colab import drive\n",
    "import weakref \n",
    "import itertools\n",
    "\n",
    "from CBM_ML import tree_importer, plot_tools, KFPF_lambda_cuts\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzKQfXfbguK"
   },
   "outputs": [],
   "source": [
    "# Colab allocates a specific RAM size to each user. To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjIxQFzU72pA"
   },
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "Lambda $(\\Lambda)$ is the most abundant strange baryon produced at FAIR energies. It weakly decays, with a branching ratio of $64 \\%$, into a proton and a negatively charged pion ($\\pi^-$). To reconstruct $\\Lambda$ all proton and $\\pi^-$ tracks are combined to make its invariant mass spectrum.\n",
    "\n",
    "In this example, we download three files that were converted to flat TTree format (simplest structure to be read by Python). The first one was generated with DCM-QGSM-SMM generator, passed through Geant4 with $p_{beam}$ = 12A Gev/c and minimum bias configuration, contains mostly signal candidates of $\\Lambda$ i.e. it contains protons and $\\pi^-$ which come from a $\\Lambda$ decay. The second one contains all proton and $\\pi^-$ 10k pairs in a 100 k events data set, generated by using URQMD generator with with $p_{beam}$ = 12A Gev/c and minimum bias configuration. The third one also contains 100 k events data set, generated by using DCM-QGSM-SMM generator with with $p_{beam}$ = 12A Gev/c and minimum bias configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoXCsOI9SgEn"
   },
   "source": [
    "Visit https://drive.google.com/drive/folders/1wve_9jaIEuYDgfUV6q_GLhW3Dn-IdCvy?usp=sharing and right click the file dcm_1m_prim_signal.root and make a shortcut from it on your drive by clicking \"Add a shortcut on my drive\". Similarly, do it for the files urqmd_100k_cleaned.root and dcm_prim_100k_cleaned.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEW51I5w8KWD"
   },
   "outputs": [],
   "source": [
    "# it will ask for your permission to use your google drive, copy the code there and paste it here. \n",
    "#the root path /presentation/treelite is the folder address on my drive. change it for your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgkKt_O27LgC"
   },
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "signal = tree_importer.tree_importer('/content/drive/MyDrive/dcm_1m_prim_signal.root','PlainTree',7)\n",
    "\n",
    "# The 100k events data set location on your drive, on my drive it is saved on the following address because I created a shortcut of it on my Drive\n",
    "df_urqmd = tree_importer.tree_importer('/content/drive/MyDrive/urqmd_100k_cleaned.root','PlainTree',7)\n",
    "\n",
    "df_dcm = tree_importer.tree_importer('/content/drive/MyDrive/dcm_prim_100k_cleaned.root','PlainTree',7)\n",
    "\n",
    "#Also call the garbage collector of python to collect unused items to free memory\n",
    "gc.collect()\n",
    "df_dcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSmy1dEU72pz"
   },
   "source": [
    "The above data frame object has some columns/features and for them at the very last column (issignal) the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ie_PTM72qH"
   },
   "source": [
    "# Selecting Background and Signal\n",
    "We have selected signal candidates only from the DCM model, therefore, we are treating it as simulated data. The URQMD data set will be treated as real experimental data.\n",
    "\n",
    "Our URQMD 100k events data, which looks more like what we will get from the final experiment, has a lot more background than signal. This problem of unequal ratio of classes (signal and background) in our data set (URQMD, 99.99% background and less than 1% signal) is called imbalance classification problem. \n",
    "\n",
    "One of the solutions to tackle this problem is resampling the data. Deleting instances from the over-represented class (in our case the background), under-sampling, is a resampling method. \n",
    "\n",
    "So for training and testing we will get signal candidates from the DCM signal and  background from URQMD (3 times signal size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUTTkp2072qI"
   },
   "outputs": [],
   "source": [
    "# We randomly choose our background and 3 times the signal\n",
    "# Similarly for the background, we select background candidates which are not in the 5 sigma region of the lambda peak\n",
    "background_selected = df_urqmd[(df_urqmd['issignal'] == 0)\n",
    "                & ((df_urqmd['mass'] > 1.07)\n",
    "                & (df_urqmd['mass'] < 1.108) | (df_urqmd['mass']>1.1227) \n",
    "                   & (df_urqmd['mass'] < 1.3))].sample(n=3*(signal.shape[0]))\n",
    "\n",
    "\n",
    "#Let's combine signal and background\n",
    "df_scaled = pd.concat([signal, background_selected])\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]\n",
    "del signal, background_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVQErryc72qc"
   },
   "outputs": [],
   "source": [
    "plot_tools.plt_sig_back(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyveX9Lh72qk"
   },
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOEQTRhb72ql"
   },
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'ldl', 'distance', 'chi2geo']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)\n",
    "\n",
    "#DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. \n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FihmBwJ72rE"
   },
   "source": [
    "## Bayesian optimization\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. For further reading visit [the git page](https://github.com/fmfn/BayesianOptimization) of the bayesian optimization used here.\n",
    "\n",
    "### Hyper parameters\n",
    "Some of the following hyper parameters will be tuned for our algorithm:\n",
    "\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL9CUKGT72rc"
   },
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.3,\n",
    "              'eval_metric': 'auc', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN20_fJV72rg"
   },
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW00bW2i72rn"
   },
   "outputs": [],
   "source": [
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOauOey772r3"
   },
   "source": [
    "# XGB models\n",
    "Now let's take the parameters selected by the bayesian optimization and apply them in our training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8eiJzKwpwXJ"
   },
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'], 'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), 'objective': 'reg:logistic'}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiTX1iSx72r9"
   },
   "outputs": [],
   "source": [
    "#To train the algorithm using the parameters selected by bayesian optimization\n",
    "#Fit/train on training data\n",
    "bst = xgb.train(param, dtrain)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict(dtrain, output_margin=False),  columns=[\"xgb_preds\"])\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2p8YPY1jRQ_"
   },
   "outputs": [],
   "source": [
    "#predictions on test set\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1, output_margin=False),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Xm-sY3W72s9"
   },
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "#ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2uJSUWAESb"
   },
   "source": [
    "## AUC and ROC\n",
    "\n",
    "The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia :\n",
    "\n",
    "“A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.”\n",
    "\n",
    "This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "\n",
    "Similarly, the function roc_auc_score computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV_FtX0MUVj-"
   },
   "source": [
    "To find the best threshold which results more signal to background ratio for lambda candidates we use the parameter the approximate median significance by the higgs boson  ML challenge (http://higgsml.lal.in2p3.fr/documentation,9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvOr9TwvkIZe"
   },
   "outputs": [],
   "source": [
    "#ROC cures for the predictions on train and test sets\n",
    "train_best, test_best = plot_tools.AMS(y_train, bst_train['xgb_preds'],y_test, bst_test['xgb_preds'])\n",
    "\n",
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "plot_tools.preds_prob(bst_test,'xgb_preds', 'issignal','test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOjnbDBbpB8r"
   },
   "source": [
    "When the AUC, best threshold and approximate median significance for train and test are nearly the same, we save that model and use it. This means that our model is general enough. In my opinion, if the test S0 is above 3.0 then it is a good enough model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eq8qfnjB2M1y"
   },
   "outputs": [],
   "source": [
    "#To save some memory on colab we delete some unused variables\n",
    "del dtrain, dtest1, x_train, x_test, y_train, y_test, df_scaled\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4k8FMgjodf_"
   },
   "source": [
    "## Applying the model on the 100k events data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgyQ0Mtq72q1"
   },
   "source": [
    "## Whole set\n",
    "We also select the selected variables from the 100k events data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbz9EdAcobhh"
   },
   "outputs": [],
   "source": [
    "x_whole_1 = df_urqmd[cuts].copy()\n",
    "y_whole_1 = pd.DataFrame(df_urqmd['issignal'], dtype='int')\n",
    "dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)\n",
    "df_urqmd['xgb_preds'] = bst.predict(dtest2, output_margin=False)\n",
    "\n",
    "del x_whole_1, y_whole_1, dtest2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9lg89Tz72q2"
   },
   "outputs": [],
   "source": [
    "x_whole = df_dcm[cuts].copy()\n",
    "y_whole = pd.DataFrame(df_dcm['issignal'], dtype='int')\n",
    "#DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. \n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "del x_whole, y_whole\n",
    "df_dcm['xgb_preds'] = bst.predict(dtest, output_margin=False)\n",
    "del dtest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOovrEVTo6Hn"
   },
   "source": [
    "The above code will return probablities for lambda candidates in the 10k events data set. Around 0 there will be more background candidates than signal and around 1 more signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loqnFETE_1W1"
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ \n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QP6WiKgQ_8yG"
   },
   "outputs": [],
   "source": [
    "#lets take the best threshold and look at the confusion matrix\n",
    "cut1 = test_best\n",
    "df_dcm['xgb_preds1'] = ((df_dcm['xgb_preds']>cut1)*1)\n",
    "cnf_matrix = confusion_matrix(df_dcm['issignal'], df_dcm['xgb_preds1'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_tools.plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "#plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOnsE6BTAfdx"
   },
   "outputs": [],
   "source": [
    "plot_tools.cut_visualization(df_urqmd,'xgb_preds',test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-9czLA72uF"
   },
   "source": [
    "## Tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IMVwtuF72uI"
   },
   "outputs": [],
   "source": [
    "xgb.to_graphviz(bst, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrItzy4C72uS"
   },
   "source": [
    "# Comparison with the manually optimized cuts of KFPF\n",
    "In the already existing Kalman Filter Particle Finder (KFPF) package for online reconstruction and selection of short-lived particles in CBM, these criteria have been manually optimized. These selection-cuts have been selected to maximize the signal to background ratio (S/B) of the $\\Lambda$ for a certain energy on a collisions generator. The selection criteria mainly depends on the collision energy, decay channel and detector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0CXPHGR72uS"
   },
   "outputs": [],
   "source": [
    "new_check_set= df_urqmd.copy()\n",
    "new_check_set['new_signal']=0\n",
    "mask1 = (new_check_set['chi2primpos'] > 18.4) & (new_check_set['chi2primneg'] > 18.4)\n",
    "\n",
    "mask2 = (new_check_set['ldl'] > 5) & (new_check_set['distance'] < 1)\n",
    "\n",
    "mask3 = (new_check_set['chi2geo'] < 3) \n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_tools.plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for KFPF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncr9MQSdD7WD"
   },
   "outputs": [],
   "source": [
    "cut3 = test_best\n",
    "mask1 = df_original['xgb_preds']>cut3\n",
    "df3=df_original[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix0Bit94QwQH"
   },
   "outputs": [],
   "source": [
    "plot_tools.comaprison_XGB_KFPF(df3['mass'],new_check_set['mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LZLqe9vwWoY"
   },
   "outputs": [],
   "source": [
    "del x,y,x_test,y_test,x_whole,y_whole,dtest,dtrain,dtest1,df3,df_clean,df_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH5o4ymExEtz"
   },
   "outputs": [],
   "source": [
    "input_tree = uproot.open('/content/drive/MyDrive/10k_events_dcmqgsm.root:PlainTree').arrays(library='np')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYUbJzgxxTH7"
   },
   "outputs": [],
   "source": [
    "df_original= pd.DataFrame(data=input_tree)\n",
    "del input_tree\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4qfipIF1f9C"
   },
   "source": [
    "# Importing the final predictor to root\n",
    "We will use the treelite library to transport the final predictor of our model to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EclXZNYd11Dc"
   },
   "outputs": [],
   "source": [
    "#install treelite \n",
    "!pip install treelite\n",
    "import treelite\n",
    "#create an object out of your model, bst in our case\n",
    "model = treelite.Model.from_xgboost(bst)\n",
    "#use GCC compiler\n",
    "toolchain = 'gcc'\n",
    "#parallel_comp can be changed upto as many processors as one have\n",
    "model.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n",
    "                 params={'parallel_comp': 8}, verbose=True)\n",
    "\n",
    "# Operating system of the target machine\n",
    "platform = 'unix'\n",
    "# C compiler to use to compile prediction code on the target machine\n",
    "toolchain = 'gcc'\n",
    "# Save the source package as a zip archive named mymodel.zip\n",
    "# Later, we'll use this package to produce the library mymodel.so.\n",
    "model.export_srcpkg(platform=platform, toolchain=toolchain,\n",
    "                    pkgpath='./mymodel.zip', libname='mymodel.so',\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha8m3Kub6UAt"
   },
   "outputs": [],
   "source": [
    "!unzip mymodel.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4DPuQAQAGVS"
   },
   "outputs": [],
   "source": [
    "#Build the source package (using GNU Make or NMake).\n",
    "%cd mymodel \n",
    "!make\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXGt5x8fpZBu"
   },
   "source": [
    "If one wants to transfer this model to a different computer, target machine, then one should follow the following commands in command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoJldomSp-bu"
   },
   "outputs": [],
   "source": [
    "sftp john@lxpool.gsi.de\n",
    "sftp> put mymodel.zip\n",
    "sftp> quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjOvqUdRqNvA"
   },
   "outputs": [],
   "source": [
    "ssh john@lxpool.gsi.de\n",
    "unzip mymodel.zip\n",
    "cd mymodel\n",
    "make -j8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpKXRlds6jpD"
   },
   "source": [
    "The following C++ code can be the applied as a macro on some new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pYC2l_r6iuO"
   },
   "outputs": [],
   "source": [
    "%%writefile test.cpp\n",
    "#include \"TROOT.h\"\n",
    "#include \"TFile.h\"\n",
    "#include \"TTree.h\"\n",
    "#include \"TH1F.h\"\n",
    "#include <vector>\n",
    "#include \"TString.h\"\n",
    "#include \"TMath.h\"\n",
    "#include \"TH1.h\"\n",
    "#include \"TF1.h\"\n",
    "#include \"TCanvas.h\"\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"main.c\"\n",
    "\n",
    "void python_to_root(){\n",
    "  TCanvas* c1 = new TCanvas();\n",
    "  //c1->Divide(2,1);\n",
    "  //c1->cd(1);\n",
    "  \n",
    "  /* Open the root file*/\n",
    "  TFile *f = new TFile(\"/gdrive/My Drive/presentations/treelite/10k_events_PFSimplePlainTree.root\",\"UPDATE\");\n",
    "  TTree *t1 = (TTree*)f->Get(\"PlainTree\");\n",
    "  \n",
    "  \n",
    "  Float_t  LambdaCandidates_chi2primneg, LambdaCandidates_chi2primpos, LambdaCandidates_ldl, LambdaCandidates_distance, LambdaCandidates_chi2geo, LambdaCandidates_mass;\n",
    "  \n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primneg\",&LambdaCandidates_chi2primneg);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primpos\",&LambdaCandidates_chi2primpos);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_ldl\",&LambdaCandidates_ldl);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_distance\",&LambdaCandidates_distance);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2geo\",&LambdaCandidates_chi2geo);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_mass\",&LambdaCandidates_mass);\n",
    "  \n",
    "  \n",
    "  std::vector<float> output{};\n",
    "  \n",
    "  const long n_entries = t1->GetEntries();\n",
    "  output.reserve(n_entries);\n",
    "\n",
    "  const size_t n_features = 5;\n",
    "  union Entry input[n_features];\n",
    "  \n",
    "  //let's create a new branch which will store the probability returened for each proton-pion pair by our model \n",
    "  Float_t probab;\n",
    "  TBranch *bpt = t1->Branch(\"probab\",&probab,\"probabilities\");\n",
    "  t1->SetBranchAddress(\"probab\",&probab);\n",
    "  \n",
    "  TH1F *h = new TH1F(\"h_prob\", \"This is the #Lambda invariant mass after the cut >0.918080\", 300, 1.04, 1.4);\n",
    "  h->GetXaxis()->SetTitle(\"[GeV]\");\n",
    "  \n",
    "  for (long i = 0; i < n_entries; i++)\n",
    "   {\n",
    "     t1->GetEntry(i);\n",
    "    \n",
    "    input[0].fvalue = LambdaCandidates_chi2primneg;\n",
    "    input[1].fvalue = LambdaCandidates_chi2primpos;\n",
    "    input[2].fvalue = LambdaCandidates_ldl;\n",
    "    input[3].fvalue = LambdaCandidates_distance;\n",
    "    input[4].fvalue = LambdaCandidates_chi2geo;\n",
    "    \n",
    "    output.push_back(predict(input, 0));\n",
    "    probab= output.at(i);\n",
    "   \n",
    "    if(probab > 0.918080){\n",
    "    h->Fill(LambdaCandidates_mass);}\n",
    "  }\n",
    "  \n",
    "  h->Draw();\n",
    "\n",
    "  \n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjofU-aMx8O7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nkyy2xmSxb2g"
   },
   "outputs": [],
   "source": [
    "# Installing root\n",
    "!mkdir -p APPS\n",
    "!pwd\n",
    "!cd APPS && wget https://root.cern/download/root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz \n",
    "!cd APPS && tar -xf root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWxtCrtk7REs"
   },
   "source": [
    "# Running colab on locally\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrsPK01b7U2W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "updated 21 june 2021 XGB for Collaboration Meeting clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
